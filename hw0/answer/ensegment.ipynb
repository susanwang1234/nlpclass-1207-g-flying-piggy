{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW0: ensegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensegment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Results of default program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "unclimatechangebody\n",
      "we are the people\n",
      "mentionyourfaves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mentionyourfaves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30secondstoearth\n",
      "current ratesoughttogodown\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "nowthatcherisdead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "howtobreakupin5words\n",
      "whatmakesgodsmile\n",
      "10peoplewhomeanalot to me\n",
      "worstdayin4words\n",
      "lovestoryin5words\n",
      "top3favouritecomics\n",
      "10breakuplines\n",
      "things thatmakeyousmile\n",
      "bestfemaleathlete\n",
      "worstbossin5words\n",
      "no wisthetimeforallgood\n",
      "i tisatruthuniversally acknowledged\n",
      "when in the course of humaneventsitbecomes necessary\n",
      "itwasabrightcoldda yinaprilandtheclocks werestrikingthirteen\n",
      "it wasthebestoftimesitw astheworstoftimesitw astheageofwisdomitwa stheageoffoolishness\n",
      "asgregorsamsaawo keonemorningfromunea sydreamshefoundhimse lftransformedinhisbe dintoagiganticinsect\n",
      "inaholeinthegroundth erelivedahobbitnotan astydirtywetholefill edwiththeendsofworms and anoozysmellnoryetadr ybaresandyholewithno thinginittositdownon or toeatitwasahobbithol eandthatmeanscomfort\n",
      "faroutintheuncharted backwaters of the unfashionable endofthewesternspira larmofthegalaxyliesa small unregardedyellowsun\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dev score of the default version is only 0.82, which is not high. And as you can see from the above result of text.txt, it’s a pretty bad segmentation. \n",
    "* After reading the material, we think it’s unreasonable to assign probability of 1/N to unseen words (if `missingfn` is not specified). The main reason is that the probability of one unseen word may be much higher than the product of probabilities of several common words. For example, P(howtobreakupin5words) > P(how) * P(to) * P(break) * P(up) * P(in) * P(5) * P(words). Product makes the joint probability smaller and smaller. \n",
    "* Thus, we need to write a new function to deal with the probabilities of unseen words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Results after 1st revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_long_words(key, N):\n",
    "    return 10./(N * 10 ** len(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to breakup in 5 words\n",
      "what makes god smile\n",
      "10 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "to p3 favourite comics\n",
      "10 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst bossin5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, the dev score becomes 1.00 and the segmentation result of text.txt file is basically correct. Our idea is that a random long sequence is less likely to be a word than a random short sequence. Thus, we reassign probabilities based on the length of unseen words. For a word of length n, we reduced the probability by n to the tenth power. \n",
    "* However, there are still some mistakes in the result of text.txt file, like “breakup”, “to p3”, “bossin5” and so on. We believe that the reason is the same as before. For example, P(breakup) > P(break) * P(up). Maybe magnitude of 10 is not accurate. What we improve next is to adjust the magnitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results after 2nd revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the function to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_long_words(key, N):\n",
    "    return 10./(N * 100000 ** len(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to breakup in 5 words\n",
      "what makes god smile\n",
      "10 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "top 3 favourite comics\n",
      "10 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst boss in 5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dev score is 1.00 and we successfully segmented “bossin5” and “to p3”. However, there are still a few mistakes. For example, “breakup” and “sitdown” should be verbs in context rather than nouns, so they are supposed to be divided into 2 words. \n",
    "* We need to learn more to figure out such a problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results after 3rd Revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the function to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_long_words(key, N):\n",
    "    log_prob = (log10(10.)-log10(N * 100000 ** len(key))) #log the probability, equivalent to log(10/(N*100000**len(key)))\n",
    "    return 10**log_prob #convert log back to probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to breakup in 5 words\n",
      "what makes god smile\n",
      "10 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "top 3 favourite comics\n",
      "10 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst boss in 5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log10 was applied to the probability assigned in avoid_long_words to try to keep all the decimals for small numbers to avoid the problem of arithmetic underflow for very small probabilities or error messages caused by the probability being too small. ALthough the words in test.txt and dev.txt are not long enough to cause extremely small probabilities that could lead to arithmetic underflow, the addition of logorithms can be used to take account of any new text files with long words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
