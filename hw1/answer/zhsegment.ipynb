{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1: zhsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entry:\n",
    "    def __init__(self, word, start_pos, log_prob, back_ptr):\n",
    "        self.word = word\n",
    "        self.start_pos = start_pos\n",
    "        self.log_prob = log_prob\n",
    "        self.back_ptr = back_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting function\n",
    "def sortbyprob(entry):\n",
    "    \"Function for sorting by probability\"\n",
    "    return entry.log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First attempt: baseline (unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pu):\n",
    "        self.Pu = Pu\n",
    "\n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "\n",
    "        # initialize the chart\n",
    "        chart = [ None for i in range(len(text))]\n",
    "        heap = []\n",
    " \n",
    "        # initialize the heap\n",
    "        # push words start from position 0\n",
    "        for i in range(len(text)):\n",
    "            word = text[: (i + 1)]\n",
    "            if word in self.Pu.keys() :\n",
    "                entry = Entry(word, 0, log10(self.Pu(word)), None)\n",
    "                heap.append(entry)\n",
    "\n",
    "        # iteratively fill in chart[i] for i\n",
    "        while heap:\n",
    "            # sort in probability descending order and pop from heap\n",
    "            heap.sort(key=sortbyprob, reverse=True)\n",
    "            entry = heap.pop(0)\n",
    "            endindex = entry.start_pos + len(entry.word) - 1\n",
    "            # compute argmax\n",
    "            if chart[endindex] is not None:\n",
    "                preventry = chart[endindex]\n",
    "                if entry.log_prob > preventry.log_prob:\n",
    "                    chart[endindex] = entry\n",
    "                else:\n",
    "                    continue \n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "            # push new entry to the heap (starting from (endindex + 1))\n",
    "            for i in range((endindex + 1), len(text)):\n",
    "                newword = text[(endindex + 1): (i + 1)]\n",
    "                if newword in self.Pu.keys() :\n",
    "                    newentry = Entry(newword, endindex + 1, entry.log_prob + log10(self.Pu(newword)), entry)\n",
    "                    if newentry not in heap:\n",
    "                        heap.append(newentry)\n",
    "\n",
    "        # get the best segmentation\n",
    "        segmentation = []\n",
    "        finalindex = len(text) - 1\n",
    "        finalentry = chart[finalindex]\n",
    "        while finalentry != None:\n",
    "            segmentation.append(finalentry.word)\n",
    "            finalentry = finalentry.back_ptr\n",
    "\n",
    "        segmentation.reverse()\n",
    "\n",
    "        return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    }
   ],
   "source": [
    "Pu = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pu) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate dev.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.48\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first attempt calculuates the probability based on P(w1,....wn) = c(w1,....,wn)/N. The problem with this is that it does not generalize well to new unseen words from the dictonary, which we also need to predict the most likely segmentation of. The bigram model will be used in the second attempt to predict for which words are more likely to get paired together with the unseen words. Unseen words in attempt 1 will not get pushed into the heap since it is not part of the dictionary, so we won't be able to compute the probability for it.\n",
    "\n",
    "* Also for the first attempt, dividing by N can cause issues where long words have greater probability than shorter words, even though shorter words are more likely to occur, so the avoid_long_words function from HW0 (for scaling down longer segments) will be applied in the second attempt as well.\n",
    "\n",
    "* Now, we scored only 0.48 in our attempt 1. Also, one sentence did not output its segmentation result because of unseen words. To deal with new words that are not in the dictionary, we are going to push unseen words with length 1 into the heap with a probability calculated by the missing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Second attempt: baseline (bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_long_words(key, N):\n",
    "    log_prob = log10(10.) - log10(N * 10000 ** len(key)) #equivalent to log(10./(N * 10000 ** len(key))) , used to avoid arithmentic underflow for small probabilities\n",
    "    return 10**log_prob #convert log back to its probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pu, Pb, lam):\n",
    "        self.Pu = Pu\n",
    "        self.Pb = Pb\n",
    "        self.lam = lam # lambda for Jelinek-Mercer Smoothing\n",
    "\n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "\n",
    "        # initialize the chart\n",
    "        chart = [ None for i in range(len(text))]\n",
    "        heap = []\n",
    " \n",
    "        # initialize the heap\n",
    "        # push words start from position 0\n",
    "        for i in range(len(text)):\n",
    "            word = text[: (i + 1)]\n",
    "            if word in self.Pu.keys() or len(word) == 1:\n",
    "                entry = Entry(word, 0, log10(self.Pu(word)), None)\n",
    "                heap.append(entry)\n",
    "\n",
    "        # iteratively fill in chart[i] for i\n",
    "        while heap:\n",
    "            # sort in probability descending order and pop from heap\n",
    "            heap.sort(key=sortbyprob, reverse=True)\n",
    "            entry = heap.pop(0)\n",
    "            endindex = entry.start_pos + len(entry.word) - 1\n",
    "            # compute argmax\n",
    "            if chart[endindex] is not None:\n",
    "                preventry = chart[endindex]\n",
    "                if entry.log_prob > preventry.log_prob:\n",
    "                    chart[endindex] = entry\n",
    "                else:\n",
    "                    continue \n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "            # push new entry to the heap (starting from (endindex + 1))\n",
    "            for i in range((endindex + 1), len(text)):\n",
    "                newword = text[(endindex + 1): (i + 1)]\n",
    "                bigram = entry.word + \" \" + newword\n",
    "                if newword in self.Pu.keys() or len(newword) == 1:\n",
    "                    # compute conditional probability (cpr)\n",
    "                    if bigram in self.Pb.keys() and entry.word in self.Pu.keys():\n",
    "                        cpr = self.Pb[bigram] / self.Pu[entry.word]\n",
    "                    else:\n",
    "                        cpr = self.Pu(newword)\n",
    "                    # apply Jelinek-Mercer Smoothing\n",
    "                    newentry = Entry(newword, endindex + 1, entry.log_prob + log10(self.lam * cpr + (1 - self.lam) * self.Pu(newword)), entry)\n",
    "                    if newentry not in heap:\n",
    "                        heap.append(newentry)\n",
    "\n",
    "        # get the best segmentation\n",
    "        segmentation = []\n",
    "        finalindex = len(text) - 1\n",
    "        finalentry = chart[finalindex]\n",
    "        while finalentry != None:\n",
    "            segmentation.append(finalentry.word)\n",
    "            finalentry = finalentry.back_ptr\n",
    "\n",
    "        segmentation.reverse()\n",
    "\n",
    "        return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    }
   ],
   "source": [
    "Pu = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "Pb = Pdist(data=datafile(\"../data/count_2w.txt\"), missingfn=lambda x, y: 0)\n",
    "lam = 0.2\n",
    "segmenter = Segment(Pu, Pb, lam) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.87\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see, there are some sentences in attempt one without output. The reason is that there are some unseen words not in the keys of built dictionary, which led to the unseen words will not be pushed into the heap. In this way, it would quit algorithm during the process. What we did is to push the unseen words with length one into the heap even though it’s not in the dictionary. \n",
    "* Besides, we added missingfn to Pdist class. The code is from our HW0.\n",
    "* Another thing we improved here is we added bigram method here and implemented Jelinek-Mercer Smoothing. If the counts of unigram and bigram both exist, we used cpr = self.Pb[bigram] / self.Pu[entry.word] to calculate maximum likelihood estimation (MLE). If not, we used cpr = self.Pu(newword) to calculate MLE. After that, we used self.lam * cpr + (1 - self.lam) * self.Pu(newword) to calculate smoothing probability.\n",
    "\n",
    "From the output we can see that, the missing sentence is shown up now. And we scored 0.87 in dev.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Third attempt: tuning parameters (lambda, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pu, Pb, lam):\n",
    "        self.Pu = Pu\n",
    "        self.Pb = Pb\n",
    "        self.lam = lam   # lambda for Jelinek-Mercer Smoothing\n",
    "\n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "\n",
    "        # initialize the chart\n",
    "        chart = [ None for i in range(len(text))]\n",
    "        heap = []\n",
    "        maxlen = 10 # max length of words (avoid too long words)\n",
    "\n",
    "        # initialize the heap\n",
    "        # push words start from position 0\n",
    "        for i in range(len(text)):\n",
    "            if i == maxlen:\n",
    "                break\n",
    "            word = text[: (i + 1)]\n",
    "            if word in self.Pu.keys() or len(word) <= 3:\n",
    "                entry = Entry(word, 0, log10(self.Pu(word)), None)\n",
    "                heap.append(entry)\n",
    "\n",
    "        # iteratively fill in chart[i] for i\n",
    "        while heap:\n",
    "            # sort in probability descending order and pop from heap\n",
    "            heap.sort(key=sortbyprob, reverse=True)\n",
    "            entry = heap.pop(0)\n",
    "            endindex = entry.start_pos + len(entry.word) - 1\n",
    "            # compute argmax\n",
    "            if chart[endindex] is not None:\n",
    "                preventry = chart[endindex]\n",
    "                if entry.log_prob > preventry.log_prob:\n",
    "                    chart[endindex] = entry\n",
    "                else:\n",
    "                    continue \n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "            # push new entry to the heap (starting from (endindex + 1))\n",
    "            for i in range((endindex + 1), len(text)):\n",
    "                if i - endindex - 1 == maxlen:\n",
    "                    break\n",
    "                newword = text[(endindex + 1): (i + 1)]\n",
    "                bigram = entry.word + \" \" + newword\n",
    "                if newword in self.Pu.keys() or len(newword) <= 3:\n",
    "                    # compute conditional probability (cpr)\n",
    "                    if bigram in self.Pb.keys() and entry.word in self.Pu.keys():\n",
    "                        cpr = self.Pb[bigram] / self.Pu[entry.word]\n",
    "                    else:\n",
    "                        cpr = self.Pu(newword)\n",
    "                    # apply Jelinek-Mercer Smoothing\n",
    "                    newentry = Entry(newword, endindex + 1, entry.log_prob + log10(self.lam * cpr + (1 - self.lam) * self.Pu(newword)), entry)\n",
    "                    if newentry not in heap:\n",
    "                        heap.append(newentry)\n",
    "\n",
    "        # get the best segmentation\n",
    "        segmentation = []\n",
    "        finalindex = len(text) - 1\n",
    "        finalentry = chart[finalindex]\n",
    "        while finalentry != None:\n",
    "            segmentation.append(finalentry.word)\n",
    "            finalentry = finalentry.back_ptr\n",
    "\n",
    "\n",
    "        segmentation.reverse()\n",
    "\n",
    "        return segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    }
   ],
   "source": [
    "Pu = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "Pb = Pdist(data=datafile(\"../data/count_2w.txt\"), missingfn=lambda x, y: 0)\n",
    "lam = 0.2425\n",
    "segmenter = Segment(Pu, Pb, lam) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.92\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third attempt, we did the following modifications:\n",
    "* Set maxlen = 10, which set a bound for the maximum length of words (too long words seems to be unilkely)\n",
    "* When pushing Entry to the words, we also push words with length <= 3 (with a very small probability) besides matched words. In this way, more possible segmentation are taken into account compared to length == 1 in our last attempt.\n",
    "* Tuning paramter $\\lambda$ for Jelinek-Mercer Smoothing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
