{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will see:\n",
    "* How this project is organized\n",
    "* How each model is trained and evaluated\n",
    "\n",
    "Please click the links offered and refer to other notebooks for more infomation of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "1. Baseline: Support Vector Classifier (SVC)\n",
    "2. Logistic Regression\n",
    "3. Multi-Layer Perceptron (MLP)\n",
    "4. Long Short-Term Memory (LSTM)\n",
    "5. Concurrent Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the code cell and outputs in [SVC](SVC.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "train_tokens = vec.fit_transform(train['comment_text'])\n",
    "test_tokens = vec.transform(test['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (tranforming the vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "global_tokens = vstack([train_tokens, test_tokens]\n",
    "# Apply Truncated Singular Vector Decomposition (SVD)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, n_iter=10)\n",
    "svd.fit(global_tokens)\n",
    "global_svd = svd.transform(global_tokens)\n",
    "# Apply MaxAbsScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(global_svd)\n",
    "global_scaled = scaler.transform(global_svd)\n",
    "train_scaled = global_scaled[:len(train)]\n",
    "test_scaled = global_scaled[len(train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for each class with pre-build SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_test = np.zeros((len(test), len(labels)))\n",
    "predict_on_train = np.zeros((len(train), len(labels)))\n",
    "for idx, label in tqdm(enumerate(labels)):\n",
    "    print(\"Training %s\"%(label))\n",
    "    m = svm.SVC(probability=True).fit(train_svd, train[label])\n",
    "    predict_on_test[:,idx] = m.predict_proba(test_svd)[:,1]\n",
    "    predict_on_train[:,idx] = m.predict_proba(train_svd)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on Kaggle = 0.73786"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement: balancing the weight of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = svm.SVC(probability=True, class_weight='balanced').fit(train_svd, train[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on Kaggle = 0.91595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that balancing the weight of each class is an effective method to solve the overfitting problem of SVC and improves the performance a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the code cell and outputs in [Logistic Regression](Logistic_Regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of Logistic Regression model and the preprocessing is the same as SVC. We just replaced the SVC model with pre-build Logistic Regression model in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LogisticRegression().fit(train_scaled, train[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "def crossValidation(clf, X, y, n=5):\n",
    "    cv = KFold(n_splits=n)\n",
    "    scores = []\n",
    "    i = 0\n",
    "    y_pred  = []\n",
    "    y_true = []\n",
    "    # split the training data to training and validation data\n",
    "    for train_index, valid_index in cv.split(X):\n",
    "        i += 1\n",
    "        X_tr, X_va, y_tr, y_va = X[train_index], X[valid_index], y.iloc[train_index], y.iloc[valid_index]\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        y_pred_sub = clf.predict(X_va)\n",
    "        newScore = clf.score(X_va, y_va)\n",
    "        scores.append(newScore)\n",
    "        newLogLoss = log_loss(y_va, y_pred_sub)\n",
    "        newROCAUC = roc_auc_score(y_va, y_pred_sub)\n",
    "        print(\"loop %d, accuracy %0.6f, logloss %0.6f, roc_auc_score %0.6f\" % (i, newScore, newLogLoss, newROCAUC))\n",
    "        # preserve one pair of y_true and y_pred\n",
    "        if i == 1:\n",
    "            y_pred = y_pred_sub\n",
    "            y_true = y_va\n",
    "    scores_array = np.asarray(scores)\n",
    "    print(\"Accuracy: %0.6f (+/- %0.6f)\" % (scores_array.mean(), scores_array.std() * 2))\n",
    "    print()\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "for i, label in tqdm(enumerate(labels)):\n",
    "    print(\"----- evaluating %s -----\" % label)\n",
    "    m = LogisticRegression()\n",
    "    y_true_sub, y_pred_sub = crossValidation(m, train_scaled, train[label])\n",
    "    y_true.append(y_true_sub)\n",
    "    y_pred.append(y_pred_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated our SVC and Logistic Regression models with Log Loss scores and ROC AUC scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on Kaggle = 0.94415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression outperform SVC and it is much more efficient to train than SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Neural Network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the code cell and outputs in [MLP](MLP.ipynb), [LSTM](LSTM.ipynb), and [CNN](CNN.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embed_size = 100\n",
    "max_features = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(train['comment_text']))\n",
    "train_input = pad_sequences(tokenizer.texts_to_sequences(train['comment_text']), maxlen = MAX_SEQUENCE_LENGTH)\n",
    "test_input = pad_sequences(tokenizer.texts_to_sequences(test['comment_text']), maxlen = MAX_SEQUENCE_LENGTH)\n",
    "examples_input = pad_sequences(tokenizer.texts_to_sequences(examples['comment_text']), maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the Tokenizer funtion to create a tokenizer based on all training comment text. The tokenizer would create indexes for each words and also would count the number of word appearance. After that, we called tokenizer.texts_to_sequences to convert each word into a sequence number(index) created by Tokenizer. And then we called pad_sequences to unify the input length, which is 100 here. If the length is smaller than 100, it would pad 0 to the sequence and make the length be 100. Otherwise, it would truncate the length. Such word sequences would be the input of our neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Reference: https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "from nltk.corpus import stopwords\n",
    "EMBEDDING_FILE = '../data/glove.6B.100d.txt'\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype = 'float32')\n",
    "\n",
    "def remove_stopwords(old_dict):\n",
    "    for key in stopwords.words():\n",
    "        if key in old_dict.keys():\n",
    "            del old_dict[key]\n",
    "    return old_dict\n",
    "    \n",
    "embedding_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE,encoding=\"utf8\"))\n",
    "\n",
    "all_embs = np.stack(embedding_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "word_index_without_sw = remove_stopwords(word_index)\n",
    "num_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embed_size))\n",
    "i = 0\n",
    "for word in word_index_without_sw.keys():\n",
    "    if i >= num_words: \n",
    "        break\n",
    "    if word in embedding_index.keys():\n",
    "        embedding_matrix[i] = embedding_index[word]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used glove.6B.100d as our word embeddings data (reference code is given). We called the custom function named get_coefs to create a dictionary for 100-dimension word embeddings. The keys are words and the values are corresponding vectors. After that, we created an embedding matrix according to tokenizer data, which would be used in the Embedding layer of neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the code cell and outputs in [MLP](MLP.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape = (MAX_SEQUENCE_LENGTH,)))\n",
    "model.add(layers.Embedding(max_features, embed_size, weights=[embedding_matrix]))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(100, activation = \"relu\"))\n",
    "model.add(layers.Dense(100, activation = \"relu\"))\n",
    "model.add(layers.Dense(100, activation = \"relu\"))\n",
    "model.add(layers.Dense(6, activation = \"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "opt = optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_input, train_labels, batch_size = BATCH_SIZE, epochs = EPOCH, validation_split = 0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the structure of our MLP. In this model, we only applied Dense layers from Keras to implement MLP, which represents fully-connected layers. At first we set the size of each input sequence as 100. After that, because our original input of neural networks is the word indexes, which is stored in train_input, we have to convert indexes into word embeddings according to our embedding_matrix. In this way, embedding layer helps us create word embeddings as the next input which can be viewed as the table used to map indexes to vectors. And then, because the size of each input is 100, the whole input would be 100 * 100, which is a 2-dimension input, while our final output would be a 1-dimension output. In this way, we have to call GlobalMaxPool1D layer to help us get a 1-dimension input. The network used ReLU activations between each hidden layer, and a sigmoid activation function would be used over the final output. The network would output 6 probabilities which corresponds to 6 classes. This model used the Adam optimizer with 0.0005 learning rate and used binary cross entropy loss over labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on Kaggle = 0.96942"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the code cell and outputs in [LSTM](LSTM.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Input(shape = (MAX_SEQUENCE_LENGTH,)))\n",
    "model.add(layers.Embedding(max_features, embed_size, weights=[embedding_matrix]))\n",
    "model.add(layers.Bidirectional(layers.LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(6, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only implementation difference between MLP and LSTM is the structure of networks. Similar to MLP, LSTM also used an input layer and an embedding layer to convert input data, and a GlobalMaxPool1D layer to process dimension. In this model, two more useful layers are used, which are dropout layers and bidirectional LSTM layers. Dropout layer is used to select a few neurons rather than all neurons from the previous layer, which is usually used to overcome overfitting problems. Bidirectional LSTM is the core layer in this model. In this layer, we also set some parameters involving dropout. ReLU activations are used between hidden layers and sigmoid activation is used over the final output. For a better comparison, this model also used the Adam optimizer with 0.0005 learning rate and used binary cross entropy loss over labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on Kaggle = 0.97206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the code cell and outputs in [CNN](CNN.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Input(shape = (MAX_SEQUENCE_LENGTH,)))\n",
    "model.add(layers.Embedding(max_features, embed_size, weights=[embedding_matrix]))\n",
    "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=1, padding='valid'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=1, padding='valid'))\n",
    "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(6, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model is CNN, which is usually used to process image data. In this model, we treated our comment texts as image data and did convolution operation. In this model, Conv1D layers and MaxPooling1D layers are added. The kernel size we set is 3 and the stride is 1 for convolution. For max pooling, the padding rule we chose is valid, whose output shape would be ((input shape - pool size + 1) / strides). But it influenced the results a lot. ReLU activations are used between convolution layers and max pooling layers and sigmoid activation is used over the final output. This model also used an Adam optimizer with 0.0005 learning rate and used binary cross entropy loss over labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on Kaggle = 0.96000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://towardsdatascience.com/machine-learning-recurrent-neural-networks-and-long-short-term-memory-lstm-python-keras-example-86001ceaaebc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'y', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'y', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our three neural networks all used the same evaluation methods, which are training accuracy, validation accuracy, training loss and validation loss. The related link is given. The figures would show us the curves of four evaluation methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}