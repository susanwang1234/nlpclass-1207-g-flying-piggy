{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('../data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=27.89\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from lexsub_check import precision\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default solution used the pre-trained word vectors to get the nearest neighbors of words. This approach disregarded the valuable information that is contained in semantic lexicons such as WordNet. And it was not good for our lexicon substitution task. We need to retrofit the vectors to make use of the semantic relations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wvec_dict(file):\n",
    "    wordVectors = {}\n",
    "    for word, vec in file:\n",
    "        wordVectors[word] = numpy.zeros(len(vec), dtype=float)\n",
    "        for idx, elem in enumerate(vec):\n",
    "            wordVectors[word][idx] = float(elem)\n",
    "        wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "    return wordVectors\n",
    "\n",
    "isNumber = re.compile(r\"\\d+.*\")\n",
    "def norm_word(word):\n",
    "    if isNumber.search(word.lower()):\n",
    "        return \"---num---\"\n",
    "    elif re.sub(r\"\\W+\", \"\", word) == \"\":\n",
    "        return \"---punc---\"\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "def build_lexicon_dict(filename):\n",
    "    lexicon = {}\n",
    "    for line in open(filename, \"r\"):\n",
    "        words = line.lower().strip().split()\n",
    "        lexicon[norm_word(words[0])]=[norm_word(word) for word in words[1:]]\n",
    "    return lexicon\n",
    "\n",
    "def retrofit_vector(wordVecs, lexicon, alpha, beta, iteration):\n",
    "    newwordVecs = deepcopy(wordVecs)\n",
    "    wvVocab = set(wordVecs.keys())\n",
    "\n",
    "    loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "    for i in range(iteration):\n",
    "        for word in loopVocab:\n",
    "            wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "            num = len(wordNeighbours)\n",
    "\n",
    "            if(num==0):\n",
    "                continue\n",
    "            numerator = alpha * wordVecs[word]\n",
    "            for neighbour in wordNeighbours:\n",
    "                numerator += beta * newwordVecs[neighbour]\n",
    "            denominator = num * beta + alpha\n",
    "            newwordVecs[word]= numerator / denominator\n",
    "    return newwordVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run baseline on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n"
     ]
    }
   ],
   "source": [
    "#lexsub = LexSub(os.path.join('../data','glove.6B.100d.magnitude'))\n",
    "lexsub = LexSub(os.path.join('../data','glove.6B.100d.retrofit.magnitude'),10)\n",
    "output = []\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=53.02\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from lexsub_check import precision\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of our baseline is to retrofit the word vectors with a graph-based approach. We regarded the words as vertices and connected the words with semantic relations. As the given graph shows, edges exist between words with semantic relations as well as the same words in Q (inferred word vector representations) and Q_hat (observed word vector representations). Our objective is to make Q close to both Q_hat (the pre-trained word vectors) in vector space and semantic lexicons. So, we constructed the Euclidean Distance formula L(Q) which computes the sum of all the edges (with weights) and take the derivative to update each element (from q1,...,qn) in Q for a number of iterations. \n",
    "\n",
    "L(Q) = sum from 1 to n(alpha_i * ||q_i - q_hat_i||^2  + sum for all edge (i,j) in E( beta_ij * ||q_i - q_j||^2))\n",
    "\n",
    "    For iterations = 1 to T:\n",
    "        For i = 1 to n:\n",
    "            if q_i has no neighbours:\n",
    "                continue\n",
    "            q_i = sum of j:(i,j) in E(beta_ij * q_j + alpha_i * q_hat_i) / sum of j:(i,j) in E( beta_ij * alpha_i)\n",
    "\n",
    "In each iteration, we iterate over each vector q_i and update their value with the formula. If some word (vertex) has no edge, we ignore it. \n",
    "\n",
    "This was shown on retrofit_vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the baseline, we tried different lexicons to train the vectors. In our implementation, we got the highest score in ppdb-xl.txt, which is 52.96 in dev.out. \n",
    "\n",
    "We tried to tune three parameters: alpha, beta and the number of iterations.\n",
    "\n",
    "Actually, when we tuned alpha or beta to other values, (e.g. alpha = 1 /1.5 /0.5, beta = 2 /0.9 /1.5), the scores are always lower than original combinations (alpha = 1 and beta = 1).\n",
    "\n",
    "When we tuned the number of iterations, the scores go higher when \"iterations\" is set as 15, while the scores go lower when \"iterations\" is set as 20 and 25. After many tries, we decided that the optimal iterations should be 15 in our implementation.\n",
    "\n",
    "Finally, we got 53.02 in dev.out.\n",
    "\n",
    "Tuning the three parameters (alpha, beta, and interations) did not improve the performance significantly. We think that we should try other loss functions or other distance functions to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
